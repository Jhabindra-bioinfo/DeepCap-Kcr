{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This python code is for training the model with label data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras import initializers,layers,regularizers\n",
    "from keras.layers import Dropout\n",
    "from keras import callbacks\n",
    "from keras.models import *\n",
    "from keras.engine.topology import Layer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout,Activation\n",
    "import numpy as np\n",
    "from keras import optimizers\n",
    "from Bio import SeqIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from scipy import interp\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU,BatchNormalization,Concatenate\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.4 (default, Aug 13 2019, 20:35:49) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version) # python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__ # keras version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding or binary encoding for each amino acids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(seq):\n",
    "    bases = ['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']\n",
    "    X = np.zeros((len(seq),len(seq[0]),len(bases)))\n",
    "    for i,m in enumerate(seq):\n",
    "        for l,s in enumerate(m):\n",
    "    #         print(s)\n",
    "            if s in bases:\n",
    "                X[i,l,bases.index(s)] = 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for reading a fasta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fasta(file_path):\n",
    "    '''File_path: Path to the fasta file\n",
    "       Returns: List of sequence\n",
    "    '''\n",
    "    one=list(SeqIO.parse(file_path,'fasta'))\n",
    "    return one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data=read_fasta('pos_fasta_lysine.txt')\n",
    "negative_data=read_fasta('neg_fasta_lysine.txt')\n",
    "data_pure=positive_data+negative_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_positive=onehot(positive_data)\n",
    "one_hot_negative=onehot(negative_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=np.concatenate((one_hot_positive,one_hot_negative),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### label the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_label=one_hot_positive.shape[0]*[[0,1]]+one_hot_negative.shape[0]*[[1,0]]\n",
    "training_label=np.asarray(training_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### capsul network codes start from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Capsnets code is modified version and origin is based on the keras implementation of CapsuleNet by Xifeng by Xifeng Guo, E-mail: `guoxifeng1990@163.com`, Github: `https://github.com/XifengGuo/CapsNet-Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Length(layers.Layer):\n",
    "    \"\"\"\n",
    "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss\n",
    "    inputs: shape=[dim_1, ..., dim_{n-1}, dim_n]\n",
    "    output: shape=[dim_1, ..., dim_{n-1}]\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squash activation for   Equation 3,  described  in the main manuscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(vectors, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param vectors: some vectors to be squashed, N-dim tensor\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same shape as input vectors\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the\n",
    "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
    "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_vector] and output shape = \\\n",
    "    [None, num_capsule, dim_vector]. For Dense Layer, input_dim_vector = dim_vector = 1.\n",
    "    :param num_capsule: number of capsules in this layer\n",
    "    :param dim_vector: dimension of the output vectors of the capsules in this layer\n",
    "    :param num_routings: number of iterations for the routing algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_capsule, dim_vector, num_routing=3,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_vector = dim_vector\n",
    "        self.num_routing = num_routing\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_vector]\"\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_vector = input_shape[2]\n",
    "\n",
    "        # Transform matrix\n",
    "        self.W = self.add_weight(\n",
    "            shape=[self.input_num_capsule, self.num_capsule, self.input_dim_vector, self.dim_vector],\n",
    "            initializer=self.kernel_initializer,\n",
    "            name='W')\n",
    "\n",
    "        # Coupling coefficient. The redundant dimensions are just to facilitate subsequent matrix calculation.\n",
    "        self.bias = self.add_weight(shape=[1, self.input_num_capsule, self.num_capsule, 1, 1],\n",
    "                                    initializer=self.bias_initializer,\n",
    "                                    name='bias',\n",
    "                                    trainable=False)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs.shape=[None, input_num_capsule, input_dim_vector]\n",
    "        # Expand dims to [None, input_num_capsule, 1, 1, input_dim_vector]\n",
    "        inputs_expand = K.expand_dims(K.expand_dims(inputs, 2), 2)\n",
    "\n",
    "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
    "        # Now it has shape = [None, input_num_capsule, num_capsule, 1, input_dim_vector]\n",
    "        inputs_tiled = K.tile(inputs_expand, [1, 1, self.num_capsule, 1, 1])\n",
    "\n",
    "        \"\"\"\n",
    "        # Begin: inputs_hat computation V1 ---------------------------------------------------------------------#\n",
    "        # Compute `inputs * W` by expanding the first dim of W. More time-consuming and need batch_size.\n",
    "        # w_tiled.shape = [batch_size, input_num_capsule, num_capsule, input_dim_vector, dim_vector]\n",
    "        w_tiled = K.tile(K.expand_dims(self.W, 0), [self.batch_size, 1, 1, 1, 1])\n",
    "        # Transformed vectors, inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
    "        inputs_hat = K.batch_dot(inputs_tiled, w_tiled, [4, 3])\n",
    "        # End: inputs_hat computation V1 ---------------------------------------------------------------------#\n",
    "        \"\"\"\n",
    "\n",
    "        # Begin: inputs_hat computation V2 ---------------------------------------------------------------------#\n",
    "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0. This is faster but requires Tensorflow.\n",
    "        # inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
    "        inputs_hat = tf.scan(lambda ac, x: K.batch_dot(x, self.W, [3, 2]),\n",
    "                             elems=inputs_tiled,\n",
    "                             initializer=K.zeros([self.input_num_capsule, self.num_capsule, 1, self.dim_vector]))\n",
    "        # End: inputs_hat computation V2 ---------------------------------------------------------------------#\n",
    "        \"\"\"\n",
    "        # Begin: routing algorithm V1, dynamic ------------------------------------------------------------#\n",
    "        def body(i, b, outputs):\n",
    "            c = tf.nn.softmax(b, dim=2)  # dim=2 is the num_capsule dimension\n",
    "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n",
    "            if i != 1:\n",
    "                b = b + K.sum(inputs_hat * outputs, -1, keepdims=True)\n",
    "            return [i-1, b, outputs]\n",
    "        cond = lambda i, b, inputs_hat: i > 0\n",
    "        loop_vars = [K.constant(self.num_routing), self.bias, K.sum(inputs_hat, 1, keepdims=True)]\n",
    "        shape_invariants = [tf.TensorShape([]),\n",
    "                            tf.TensorShape([None, self.input_num_capsule, self.num_capsule, 1, 1]),\n",
    "                            tf.TensorShape([None, 1, self.num_capsule, 1, self.dim_vector])]\n",
    "        _, _, outputs = tf.while_loop(cond, body, loop_vars, shape_invariants)\n",
    "        # End: routing by aggrement  algorithm 1, dynamic ------------------------------------------------------------#\n",
    "        \"\"\"\n",
    "\n",
    "        # Begin: routing algorithm V2, static -----------------------------------------------------------#\n",
    "        # Routing algorithm V2. Use iteration. V2 and V1 both work without much difference on performance\n",
    "        assert self.num_routing > 0, 'The num_routing should be > 0.'\n",
    "        for i in range(self.num_routing):\n",
    "            c = tf.nn.softmax(self.bias, dim=2)\n",
    "            \n",
    "            # dim=2 is the num_capsule dimension\n",
    "            # outputs.shape=[None, 1, num_capsule, 1, dim_vector]\n",
    "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n",
    "\n",
    "            # last iteration needs not compute bias which will not be passed to the graph any more anyway.\n",
    "            if i != self.num_routing - 1:\n",
    "                # self.bias = K.update_add(self.bias, K.sum(inputs_hat * outputs, [0, -1], keepdims=True))\n",
    "                self.bias += K.sum(inputs_hat * outputs, -1, keepdims=True)\n",
    "                # tf.summary.histogram('BigBee', self.bias)  # for debugging\n",
    "        # End: routing algorithm V2, static ------------------------------------------------------------#\n",
    "\n",
    "        return K.reshape(outputs, [-1, self.num_capsule, self.dim_vector])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, self.num_capsule, self.dim_vector])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrimaryCap(inputs, dim_vector, n_channels, kernel_size, strides, padding):\n",
    "    \"\"\"\n",
    "    Apply Conv1D `n_channels` times and concatenate all capsules\n",
    "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
    "    :param dim_vector: the dim of the output vector of capsule\n",
    "    :param n_channels: the number of types of capsules\n",
    "    :return: output tensor, shape=[None, num_capsule, dim_vector]\n",
    "    \"\"\"\n",
    "    output = layers.Conv1D(filters=dim_vector * n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n",
    "                           name='primarycap_conv1d')(inputs)\n",
    "    outputs = layers.Reshape(target_shape=[-1, dim_vector], name='primarycap_reshape')(output)\n",
    "    return layers.Lambda(squash, name='primarycap_squash')(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CapsNet(input_shape, n_class, num_routing):\n",
    "\n",
    "    x = layers.Input(shape=input_shape)\n",
    "    conv1 = layers.Conv1D(filters=32, kernel_size=7, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
    "    conv1=Dropout(0.7)(conv1)\n",
    "    lstm = layers.LSTM(units=128,   name='Lstm',  return_sequences=True)(conv1)\n",
    "    # Layer 2: Conv1D layer with `squash` activation, then reshape to [None, num_capsule, dim_vector]\n",
    "    primarycaps = PrimaryCap(lstm, dim_vector=8, n_channels=16, kernel_size=3, strides=1, padding='valid')\n",
    "\n",
    "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
    "    KcrCaps = CapsuleLayer(num_capsule=n_class, dim_vector=8, num_routing=num_routing, name='KcrCaps')(primarycaps)\n",
    "\n",
    "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
    "    # If using tensorflow, this will not be necessary. :)\n",
    "    out = Length(name='capsnet')(KcrCaps)\n",
    "    #model\n",
    "    train_model = Model(x, out)\n",
    "    return train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "     When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
    "    :param y_true: [None, n_classes]\n",
    "    :param y_pred: [None, num_capsule]\n",
    "    :return: a scalar loss value.\n",
    "    \"\"\"\n",
    "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    "\n",
    "    return K.mean(K.sum(L, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our DeepCap-Kcr model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 31, 20)            0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 25, 32)            4512      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25, 32)            0         \n",
      "_________________________________________________________________\n",
      "Lstm (LSTM)                  (None, 25, 128)           82432     \n",
      "_________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)   (None, 23, 128)           49280     \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 368, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 368, 8)            0         \n",
      "_________________________________________________________________\n",
      "KcrCaps (CapsuleLayer)       (None, 2, 8)              47840     \n",
      "_________________________________________________________________\n",
      "capsnet (Length)             (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 184,064\n",
      "Trainable params: 183,328\n",
      "Non-trainable params: 736\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#31 is length of each sequence and 20 is the one-hot encoding vector for each amino acids, which means we used 31 lenthg of sequence\n",
    "mode1= CapsNet(input_shape=(31,20),n_class=2,num_routing=3)\n",
    "mode1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-fold cross validation\n",
    "kf= KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores=[]\n",
    "all_preds=[]\n",
    "conf_mat=[]\n",
    "reports=[]\n",
    "roc=[]\n",
    "roc_auc=[]\n",
    "tprs=[]\n",
    "for i,(train_index,test_index) in enumerate(kf.split(training_data)):\n",
    "    x_train, x_test=training_data[train_index],training_data[test_index]\n",
    "    y_train, y_test=training_label[train_index],training_label[test_index]\n",
    "    y_train=np.asarray(y_train)\n",
    "    mode1= CapsNet(input_shape=(31,20),n_class=2,num_routing=3)\n",
    "    mode1.compile(optimizers.RMSprop(lr=0.0001),loss=margin_loss,metrics=['acc'])\n",
    "    call=[keras.callbacks.EarlyStopping(monitor='val_loss',patience=25)]\n",
    "    history=mode1.fit(x=x_train,y=y_train,batch_size=128,epochs=500,validation_data=[x_test,y_test], callbacks=call)\n",
    "   # mode1.save('/home/jk/jeevan_capnet/'+str(i)+'Lastepoch_new.h5') # save your model\n",
    "    plt.plot(history.history['loss'],label='train'+str(i))\n",
    "    scores=mode1.evaluate(x_test,y_test)\n",
    "    all_scores.append(scores)\n",
    "    pred=mode1.predict(x_test)\n",
    "    all_preds.append(pred)\n",
    "    y_prediction=np.argmax(pred,1)\n",
    "    y_true=np.argmax(y_test,1)\n",
    "    conf= metrics.confusion_matrix(y_true,y_prediction)\n",
    "    print(conf)\n",
    "    conf_mat.append(conf)\n",
    "    report=metrics.classification_report(y_true,y_prediction)\n",
    "    reports.append(report)\n",
    "    fpr,tpr,_=metrics.roc_curve(y_true=y_test[:,1],y_score=pred[:,1])\n",
    "    roc.append([fpr,tpr])\n",
    "    roc_auc.append(metrics.auc(fpr,tpr))\n",
    "    tpr=interp(base_fpr,fpr,tpr)\n",
    "    tpr[0]=0.0\n",
    "    tprs.append(tpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_fpr = np.linspace(0, 1, 101)\n",
    "base_fpr[-1]=1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot ROC for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fprs=[]\n",
    "for i,[fpr,tpr] in enumerate(roc):\n",
    "    fprs.append(fpr)\n",
    "    plt.plot(fpr,tpr,label='ROC fold {} (AUC = {:.4f})'.format(i,roc_auc[i]),lw=1,alpha=0.3)\n",
    "plt.plot(base_fpr, np.average(tprs,axis=0),\n",
    "        label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (np.mean(roc_auc),np.std(roc_auc)),lw=1,alpha=.8,color='b')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=1, alpha=.8,color='c')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate',fontweight='bold')\n",
    "plt.ylabel('True Positive Rate',fontweight='bold')\n",
    "plt.savefig( 'five_fold_roc_capsulnet_lstm.pdf') # use your own dictionay \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot train and validadion loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pyplot.plot(history.history['loss'], label='Train')\n",
    "pyplot.plot(history.history['val_loss'], label='Test')\n",
    "pyplot.legend()\n",
    "plt.xlabel('Epoch Number',fontweight='bold')\n",
    "pyplot.savefig('train_loss_lstm.pdf')\n",
    "pyplot.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
