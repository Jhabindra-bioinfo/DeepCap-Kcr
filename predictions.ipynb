{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This python code is for prediction, we do not label the data, we just want to know the the given sequence is either Kcr(positive) or Non-Kcr (Negative), If the model predict >0.5, which means the sequence contains Kcr modifications "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the python libraries, we used python version 3.6 and keras version 2.1.1 using TensorFlow backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras import initializers,layers,regularizers\n",
    "from keras.layers import Dropout\n",
    "from keras import callbacks\n",
    "from keras.models import *\n",
    "from keras.engine.topology import Layer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout,Activation\n",
    "import numpy as np\n",
    "from keras import optimizers\n",
    "from Bio import SeqIO\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from scipy import interp\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU,BatchNormalization,Concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding or binary encoding for each amino acids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(seq):\n",
    "    bases = ['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']\n",
    "    X = np.zeros((len(seq),len(seq[0]),len(bases)))\n",
    "    for i,m in enumerate(seq):\n",
    "        for l,s in enumerate(m):\n",
    "    #         print(s)\n",
    "            if s in bases:\n",
    "                X[i,l,bases.index(s)] = 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for reading a fasta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fasta(file_path):\n",
    "    '''File_path: Path to the fasta file\n",
    "       Returns: List of sequence\n",
    "    '''\n",
    "    one=list(SeqIO.parse(file_path,'fasta'))\n",
    "    return one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### capsul network codes start from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Capsnets code is modified version and origin is based on the keras implementation of CapsuleNet by Xifeng by Xifeng Guo, E-mail: `guoxifeng1990@163.com`, Github: `https://github.com/XifengGuo/CapsNet-Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Length(layers.Layer):\n",
    "    \"\"\"\n",
    "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss\n",
    "    inputs: shape=[dim_1, ..., dim_{n-1}, dim_n]\n",
    "    output: shape=[dim_1, ..., dim_{n-1}]\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squash activation for   Equation 3,  described  in the main manuscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(vectors, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param vectors: some vectors to be squashed, N-dim tensor\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same shape as input vectors\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the\n",
    "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
    "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_vector] and output shape = \\\n",
    "    [None, num_capsule, dim_vector]. For Dense Layer, input_dim_vector = dim_vector = 1.\n",
    "    :param num_capsule: number of capsules in this layer\n",
    "    :param dim_vector: dimension of the output vectors of the capsules in this layer\n",
    "    :param num_routings: number of iterations for the routing algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_capsule, dim_vector, num_routing=3,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_vector = dim_vector\n",
    "        self.num_routing = num_routing\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_vector]\"\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_vector = input_shape[2]\n",
    "\n",
    "        # Transform matrix\n",
    "        self.W = self.add_weight(\n",
    "            shape=[self.input_num_capsule, self.num_capsule, self.input_dim_vector, self.dim_vector],\n",
    "            initializer=self.kernel_initializer,\n",
    "            name='W')\n",
    "\n",
    "        # Coupling coefficient. The redundant dimensions are just to facilitate subsequent matrix calculation.\n",
    "        self.bias = self.add_weight(shape=[1, self.input_num_capsule, self.num_capsule, 1, 1],\n",
    "                                    initializer=self.bias_initializer,\n",
    "                                    name='bias',\n",
    "                                    trainable=False)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs.shape=[None, input_num_capsule, input_dim_vector]\n",
    "        # Expand dims to [None, input_num_capsule, 1, 1, input_dim_vector]\n",
    "        inputs_expand = K.expand_dims(K.expand_dims(inputs, 2), 2)\n",
    "\n",
    "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
    "        # Now it has shape = [None, input_num_capsule, num_capsule, 1, input_dim_vector]\n",
    "        inputs_tiled = K.tile(inputs_expand, [1, 1, self.num_capsule, 1, 1])\n",
    "\n",
    "        \"\"\"\n",
    "        # Begin: inputs_hat computation V1 ---------------------------------------------------------------------#\n",
    "        # Compute `inputs * W` by expanding the first dim of W. More time-consuming and need batch_size.\n",
    "        # w_tiled.shape = [batch_size, input_num_capsule, num_capsule, input_dim_vector, dim_vector]\n",
    "        w_tiled = K.tile(K.expand_dims(self.W, 0), [self.batch_size, 1, 1, 1, 1])\n",
    "        # Transformed vectors, inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
    "        inputs_hat = K.batch_dot(inputs_tiled, w_tiled, [4, 3])\n",
    "        # End: inputs_hat computation V1 ---------------------------------------------------------------------#\n",
    "        \"\"\"\n",
    "\n",
    "        # Begin: inputs_hat computation V2 ---------------------------------------------------------------------#\n",
    "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0. This is faster but requires Tensorflow.\n",
    "        # inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
    "        inputs_hat = tf.scan(lambda ac, x: K.batch_dot(x, self.W, [3, 2]),\n",
    "                             elems=inputs_tiled,\n",
    "                             initializer=K.zeros([self.input_num_capsule, self.num_capsule, 1, self.dim_vector]))\n",
    "        # End: inputs_hat computation V2 ---------------------------------------------------------------------#\n",
    "        \"\"\"\n",
    "        # Begin: routing algorithm V1, dynamic ------------------------------------------------------------#\n",
    "        def body(i, b, outputs):\n",
    "            c = tf.nn.softmax(b, dim=2)  # dim=2 is the num_capsule dimension\n",
    "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n",
    "            if i != 1:\n",
    "                b = b + K.sum(inputs_hat * outputs, -1, keepdims=True)\n",
    "            return [i-1, b, outputs]\n",
    "        cond = lambda i, b, inputs_hat: i > 0\n",
    "        loop_vars = [K.constant(self.num_routing), self.bias, K.sum(inputs_hat, 1, keepdims=True)]\n",
    "        shape_invariants = [tf.TensorShape([]),\n",
    "                            tf.TensorShape([None, self.input_num_capsule, self.num_capsule, 1, 1]),\n",
    "                            tf.TensorShape([None, 1, self.num_capsule, 1, self.dim_vector])]\n",
    "        _, _, outputs = tf.while_loop(cond, body, loop_vars, shape_invariants)\n",
    "        # End: routing by aggrement  algorithm 1, dynamic ------------------------------------------------------------#\n",
    "        \"\"\"\n",
    "\n",
    "        # Begin: routing algorithm V2, static -----------------------------------------------------------#\n",
    "        # Routing algorithm V2. Use iteration. V2 and V1 both work without much difference on performance\n",
    "        assert self.num_routing > 0, 'The num_routing should be > 0.'\n",
    "        for i in range(self.num_routing):\n",
    "            c = tf.nn.softmax(self.bias, dim=2)\n",
    "            \n",
    "            # dim=2 is the num_capsule dimension\n",
    "            # outputs.shape=[None, 1, num_capsule, 1, dim_vector]\n",
    "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n",
    "\n",
    "            # last iteration needs not compute bias which will not be passed to the graph any more anyway.\n",
    "            if i != self.num_routing - 1:\n",
    "                # self.bias = K.update_add(self.bias, K.sum(inputs_hat * outputs, [0, -1], keepdims=True))\n",
    "                self.bias += K.sum(inputs_hat * outputs, -1, keepdims=True)\n",
    "                # tf.summary.histogram('BigBee', self.bias)  # for debugging\n",
    "        # End: routing algorithm V2, static ------------------------------------------------------------#\n",
    "\n",
    "        return K.reshape(outputs, [-1, self.num_capsule, self.dim_vector])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, self.num_capsule, self.dim_vector])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrimaryCap(inputs, dim_vector, n_channels, kernel_size, strides, padding):\n",
    "    \"\"\"\n",
    "    Apply Conv1D `n_channels` times and concatenate all capsules\n",
    "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
    "    :param dim_vector: the dim of the output vector of capsule\n",
    "    :param n_channels: the number of types of capsules\n",
    "    :return: output tensor, shape=[None, num_capsule, dim_vector]\n",
    "    \"\"\"\n",
    "    output = layers.Conv1D(filters=dim_vector * n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n",
    "                           name='primarycap_conv1d')(inputs)\n",
    "    outputs = layers.Reshape(target_shape=[-1, dim_vector], name='primarycap_reshape')(output)\n",
    "    return layers.Lambda(squash, name='primarycap_squash')(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CapsNet(input_shape, n_class, num_routing):\n",
    "\n",
    "    x = layers.Input(shape=input_shape)\n",
    "    conv1 = layers.Conv1D(filters=32, kernel_size=7, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
    "    conv1=Dropout(0.7)(conv1)\n",
    "    lstm = layers.LSTM(units=128,   name='Lstm',  return_sequences=True)(conv1)\n",
    "    # Layer 2: Conv1D layer with `squash` activation, then reshape to [None, num_capsule, dim_vector]\n",
    "    primarycaps = PrimaryCap(lstm, dim_vector=8, n_channels=16, kernel_size=3, strides=1, padding='valid')\n",
    "\n",
    "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
    "    KcrCaps = CapsuleLayer(num_capsule=n_class, dim_vector=8, num_routing=num_routing, name='KcrCaps')(primarycaps)\n",
    "\n",
    "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
    "    # If using tensorflow, this will not be necessary. :)\n",
    "    out = Length(name='capsnet')(KcrCaps)\n",
    "    #model\n",
    "    train_model = Model(x, out)\n",
    "    return train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "     When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
    "    :param y_true: [None, n_classes]\n",
    "    :param y_pred: [None, num_capsule]\n",
    "    :return: a scalar loss value.\n",
    "    \"\"\"\n",
    "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    "\n",
    "    return K.mean(K.sum(L, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our DeepCap-Kcr model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 31, 20)            0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 25, 32)            4512      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 25, 32)            0         \n",
      "_________________________________________________________________\n",
      "Lstm (LSTM)                  (None, 25, 128)           82432     \n",
      "_________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)   (None, 23, 128)           49280     \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 368, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 368, 8)            0         \n",
      "_________________________________________________________________\n",
      "KcrCaps (CapsuleLayer)       (None, 2, 8)              47840     \n",
      "_________________________________________________________________\n",
      "capsnet (Length)             (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 184,064\n",
      "Trainable params: 183,328\n",
      "Non-trainable params: 736\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#31 is length of each sequence and 20 is the one-hot encoding vector for each amino acids, which means we used 31 lenthg of sequence\n",
    "mode1= CapsNet(input_shape=(31,20),n_class=2,num_routing=3)\n",
    "mode1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We Uploaded  and read the independent data set ( as described in Materials and methods (dataset) in the manuscript) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is for prediction using the trained five fold models named str (i)Lateopch_new.h5\n",
    "def get_pred():\n",
    "    positive_ind_data=read_fasta('pos_ind_testdata.txt') # positive data # use read_rasta function written in the beginning\n",
    "    print(\"The total length of positive sequences to be predicted is:\", len(positive_ind_data))\n",
    "    negative_ind_data=read_fasta('neg_ind_testdata.txt') #negative data\n",
    "    print(\"The total length of negative sequences to be predicted is:\", len(negative_ind_data))\n",
    "    all_ind_data=positive_ind_data+negative_ind_data\n",
    "    all_ind_onehot=onehot(all_ind_data) # use onehot function  written in the beginning\n",
    "    all_pred_y=[]\n",
    "    for i in range(5):\n",
    "        mode1.load_weights(str(i+0)+'Lastepoch_new'+'.h5')\n",
    "        pred_y=mode1.predict(all_ind_onehot)\n",
    "        #all_pred_y.append(pred_y)\n",
    "        pred_y=np.argmax(pred_y, axis=1)\n",
    "        all_pred_y.append(pred_y)\n",
    "        print(pred_y)\n",
    "    all_pred_y=np.average(all_pred_y,axis=0)\n",
    "    print(all_pred_y)\n",
    "    return all_pred_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total length of positive sequences to be predicted is: 2989\n",
      "The total length of negative sequences to be predicted is: 2989\n",
      "[1 1 1 ... 1 0 0]\n",
      "[1 1 1 ... 1 0 0]\n",
      "[1 1 0 ... 0 0 0]\n",
      "[1 1 1 ... 0 0 0]\n",
      "[1 1 1 ... 0 0 0]\n",
      "[1.  1.  0.8 ... 0.4 0.  0. ]\n"
     ]
    }
   ],
   "source": [
    "# result for each fold , 1's is for Kcr sites and 0's is for non-Kcr sites\n",
    "result=get_pred()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result shows prediction for first 1000 positive sequence :  \n",
      " [1.  1.  0.8 0.  1.  0.  0.  1.  1.  1.  0.8 1.  0.4 1.  1.  1.  1.  1.\n",
      " 0.8 1.  1.  1.  1.  1.  1.  1.  1.  0.2 1.  1.  1.  1.  1.  1.  1.  1.\n",
      " 0.8 0.8 1.  1.  1.  0.8 1.  1.  1.  1.  1.  0.4 1.  1.  1.  1.  0.4 1.\n",
      " 1.  1.  1.  1.  0.2 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      " 0.8 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.6 1.  1.  1.  1.  1.  0.8\n",
      " 1.  0.8 0.6 1.  1.  1.  1.  1.  1.  1.  1.  1.  0.8 1.  1.  1.  1.  0.2\n",
      " 1.  0.6 1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  0.\n",
      " 1.  1.  1.  1.  1.  0.8 1.  1.  1.  1.  1.  1.  1.  1.  1.  0.4 0.  1.\n",
      " 1.  1.  1.  1.  1.  1.  1.  1.  0.4 1.  0.8 1.  1.  0.8 1.  1.  0.8 1.\n",
      " 1.  1.  1.  1.  0.4 1.  1.  0.4 1.  1.  1.  1.  0.8 0.4 1.  1.  0.  1.\n",
      " 1.  1.  0.8 1.  1.  1.  1.  1.  0.2 0.2 1.  1.  0.6 1.  1.  1.  1.  1.\n",
      " 1.  0.6 1.  1.  1.  1.  1.  0.  0.2 1.  1.  1.  1.  1.  0.2 1.  1.  0.6\n",
      " 1.  1.  1.  1.  1.  1.  0.  1.  0.  0.  0.6 1.  1.  1.  1.  1.  1.  0.2\n",
      " 1.  1.  1.  1.  1.  1.  0.2 1.  1.  1.  1.  0.  0.4 1.  1.  1.  1.  1.\n",
      " 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.8 1.  1.  0.4\n",
      " 1.  0.4 1.  1.  1.  0.6 1.  0.  1.  0.6 1.  1.  1.  1.  0.8 0.  1.  1.\n",
      " 1.  1.  1.  1.  0.  1.  1.  1.  0.2 1.  1.  0.2 1.  0.8 1.  1.  1.  1.\n",
      " 1.  1.  0.6 1.  1.  1.  1.  1.  1.  0.2 1.  1.  0.4 1.  1.  1.  0.8 1.\n",
      " 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.8 0.4 1.  1.  1.  1.  1.\n",
      " 0.  0.8 1.  1.  0.4 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      " 1.  0.4 1.  1.  1.  1.  1.  1.  1.  0.2 1.  1.  1.  1.  0.4 0.8 1.  1.\n",
      " 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.6 0.2 0.8 1.\n",
      " 1.  1.  1.  1.  1.  0.6 1.  0.8 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      " 1.  1.  0.4 1.  1.  1.  1.  1.  1.  1.  1.  0.6 1.  0.4 1.  0.  0.  1.\n",
      " 1.  0.4 1.  1.  1.  1.  1.  1.  1.  0.2 0.8 1.  1.  0.8 1.  1.  1.  1.\n",
      " 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  0.\n",
      " 1.  1.  1.  0.8 1.  1.  0.6 1.  1.  1.  1.  0.2 1.  1.  1.  1.  1.  1.\n",
      " 0.  1.  1.  1.  1.  1.  0.4 1.  1.  1.  0.8 1.  0.8 1.  1.  1.  1.  1.\n",
      " 1.  1.  1.  1.  1.  1.  1.  1.  1.  0.6 1.  1.  1.  1.  1.  1.  1.  1.\n",
      " 0.  1.  1.  1.  1.  1.  1.  1.  0.8 1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      " 0.  1.  1.  1.  1.  1.  0.8 1.  1.  1.  0.6 1.  0.4 1.  1.  1.  1.  0.6\n",
      " 0.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  0.4 0.8 1.  1.  1.\n",
      " 1.  1.  0.  0.  0.4 1.  1.  1.  1.  1.  0.  1.  1.  0.  1.  1.  0.8 1.\n",
      " 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.8 1.  1.  1.  1.  0.  0.\n",
      " 1.  1.  1.  0.4 1.  1.  0.2 1.  1.  1.  1.  1.  1.  1.  0.6 1.  1.  1.\n",
      " 1.  1.  1.  1.  0.8 1.  1.  0.4 1.  1.  1.  0.8 1.  1.  1.  1.  1.  0.4\n",
      " 0.8 1.  1.  1.  1.  0.  0.  1.  1.  0.2 1.  1.  1.  1.  1.  1.  1.  1.\n",
      " 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.6 0.8 1.\n",
      " 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.8 1.  1.  1.  1.\n",
      " 1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  0.8 0.  0.4 1.  0.  1.  1.  1.\n",
      " 1.  1.  1.  1.  1.  0.6 1.  1.  1.  1.  1.  1.  1.  0.8 1.  0.  0.8 1.\n",
      " 0.6 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.4 1.  1.  1.  1.  1.  1.\n",
      " 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.8 1.  1.  1.  0.2 1.\n",
      " 1.  1.  1.  1.  1.  0.2 0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      " 1.  1.  1.  1.  1.  1.  1.  1.  1.  0.2 1.  1.  1.  1.  0.8 1.  1.  1.\n",
      " 1.  1.  1.  0.6 1.  1.  0.4 0.4 1.  1.  1.  1.  1.  1.  1.  1.  1.  0.4\n",
      " 1.  1.  1.  1.  1.  1.  1.  1.  1.  0.8 1.  1.  1.  1.  1.  1.  0.  1.\n",
      " 1.  0.6 0.6 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.6 0.6 0.  1.\n",
      " 1.  1.  1.  1.  1.  1.  1.  0.8 1.  1.  1.  1.  1.  1.  1.  1.  0.  0.6\n",
      " 1.  1.  1.  1.  1.  1.  1.  1.  0.8 1.  1.  1.  1.  0.8 0.  0.  1.  1.\n",
      " 1.  1.  0.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      " 1.  0.8 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.\n",
      " 1.  1.  1.  0.8 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.2 0.6 1.  1.\n",
      " 0.  1.  1.  1.  0.4 1.  1.  1.  0.  1.  0.8 1.  1.  0.8 1.  1.  1.  1.\n",
      " 0.4 1.  0.8 1.  1.  1.  1.  1.  1.  0.  0.8 1.  1.  1.  1.  1.  0.8 1.\n",
      " 1.  1.  0.  1.  0.  0.  1.  0.  1.  0. ]\n"
     ]
    }
   ],
   "source": [
    "print(\"The result shows prediction for first 1000 positive sequence :\",' \\n', result[:1000]) # prediction of first 10000 positive samples, results shows the accurate prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result shows prediction for last 1000 negative sequence :  \n",
      " jk [0.  0.  0.  0.  0.  0.2 0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.8 0.4\n",
      " 0.  0.  0.  0.2 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.8 0.  0.  0.  1.\n",
      " 0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.  0.  0.  0.8 0.  0.  0.2 0.  0.2\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.6 0.2 0.2 0.4 1.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.  0.8 0.  0.  0.  0.8 0.  1.  0.  0.4 0.\n",
      " 0.6 0.  0.4 0.4 0.2 0.  1.  0.  1.  1.  0.  0.  0.  0.  0.  0.6 0.  0.\n",
      " 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.2 0.  0.  0.8 0.  0.  0.  0.  0.\n",
      " 0.  0.2 0.  0.  0.  0.8 0.  0.  0.2 0.6 0.4 0.  0.  0.  0.  0.  0.2 0.\n",
      " 0.  0.  0.  0.  0.  0.  0.8 0.  0.  0.6 0.  0.4 0.  0.  0.  0.  0.  0.\n",
      " 0.  1.  0.  0.  0.8 0.  0.  0.  0.  0.  0.8 0.  0.  0.  0.  0.  0.  0.4\n",
      " 0.  0.  0.8 0.  0.  0.4 0.  1.  0.  0.  0.  0.  0.  0.2 0.  0.4 0.8 0.\n",
      " 0.  0.  1.  0.  0.  0.  0.  0.  0.6 0.  0.4 0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.  0.  0.4 0.4 0.  1.  1.  0.  0.2 0.  0.2 0.  0.4 0.  0.  0.8 0.\n",
      " 0.4 0.  0.  0.  0.  0.  1.  0.  0.  0.8 1.  0.  1.  0.  0.  0.2 0.  0.6\n",
      " 0.  0.  0.8 0.  1.  0.  1.  0.  0.  0.  0.6 0.  0.  1.  0.  0.  0.  0.\n",
      " 1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.2 0.  0.  0.\n",
      " 0.  0.2 0.  0.  1.  0.  1.  0.  0.  0.2 0.  0.  0.  0.2 0.  0.  0.  0.\n",
      " 0.  0.8 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.4 0.6 0.\n",
      " 0.  0.  0.  0.  1.  0.  0.8 0.2 1.  0.  0.  1.  0.  1.  0.6 0.  0.  0.\n",
      " 1.  0.2 0.  0.  0.  1.  0.8 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      " 0.  1.  0.8 0.  0.  0.6 0.  0.  1.  0.  0.  0.8 0.2 0.  1.  0.8 0.  0.\n",
      " 1.  0.2 0.  1.  1.  0.6 0.  0.6 0.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      " 0.6 1.  0.  0.4 0.  0.  0.2 0.  0.  0.  0.  0.  0.  0.2 0.  1.  0.  1.\n",
      " 1.  0.  0.4 0.  0.  0.  0.  0.  0.  1.  1.  0.6 0.  0.8 0.  0.  1.  0.\n",
      " 0.  0.  0.  0.  1.  0.  0.  0.4 0.  1.  0.  0.  0.  0.6 0.4 0.  0.6 1.\n",
      " 0.  0.  0.  0.  1.  1.  0.  0.8 0.  1.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      " 0.6 0.  0.  0.2 1.  0.8 1.  0.  0.2 0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.8 0.  0.2 0.  1.  0.  1.  0.  0.  0.  0.  0.6 0.\n",
      " 0.  0.  0.  0.  0.2 0.  0.2 0.  0.  0.  0.  0.  0.  0.  0.  0.6 0.  0.2\n",
      " 0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.  1.  0.  0.\n",
      " 0.6 0.8 0.  0.  0.  1.  0.  0.6 0.  0.  0.  0.  0.6 0.  0.  0.6 0.  0.\n",
      " 0.  0.  0.2 0.  0.4 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      " 0.6 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.\n",
      " 0.  0.8 0.6 0.  1.  0.  0.  0.  0.  0.  0.4 1.  0.  0.  1.  0.  0.  0.\n",
      " 1.  0.  1.  0.  0.2 0.  0.  0.2 0.  1.  0.  0.  0.8 0.  1.  0.  0.  0.\n",
      " 1.  0.  0.  0.  0.2 0.  0.  0.4 0.  1.  0.8 0.  0.  0.  0.  0.2 0.  0.\n",
      " 0.  0.8 1.  0.4 0.8 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.4 0.  0.2 0.\n",
      " 0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.6 0.  1.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.4 0.  0.  0.  1.  0.  0.  0.  0.\n",
      " 0.  1.  0.2 0.  1.  0.  0.2 0.  0.8 1.  0.  0.  0.  0.  0.8 0.2 0.  0.\n",
      " 1.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.8\n",
      " 0.  0.  0.2 1.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.6 0.  0.  1.  1.\n",
      " 0.  0.  1.  1.  0.  0.  0.6 0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.2\n",
      " 0.  1.  1.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  1.  0.8 0.  0.  0.  1.  0.  0.2 1.  1.  0.  0.  1.\n",
      " 0.  0.  0.4 1.  0.4 0.  0.  0.  0.  1.  1.  0.4 0.  0.  0.  0.8 0.  0.\n",
      " 0.  0.  0.  1.  0.  0.  0.4 1.  0.  0.  1.  0.4 0.  0.  0.  0.4 0.4 0.\n",
      " 0.  0.4 0.  0.  0.  0.8 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      " 0.8 0.  0.  1.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.2 0.  0.  0.6 1.\n",
      " 0.2 1.  0.  0.  0.4 0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.8 0.\n",
      " 0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.2 0.  0.  0.2 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.4 0.  0.  0.\n",
      " 1.  0.  1.  0.  0.6 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      " 0.  0.8 0.2 0.  0.6 0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.8 1.\n",
      " 0.  0.  0.8 0.6 0.  0.2 0.6 0.  0.  0.  0.  0.  0.2 1.  0.  0.  0.8 0.\n",
      " 0.  0.  0.  0.  0.  0.  0.  0.4 0.  0. ]\n"
     ]
    }
   ],
   "source": [
    "print(\"The result shows prediction for last 1000 negative sequence :\",' \\n',\"jk\", result[-1000:]) # prediction of last 10000 negative samples, results shows the accurate prediction "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
